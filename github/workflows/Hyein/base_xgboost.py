import xgboost as xgb
from xgboost import XGBRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score, mean_squared_error
import pandas as pd
import numpy as np

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from kan.custom_processing import remove_outliers_iqr
import torch
import os

root_dir = os.path.join(os.getcwd(), 'github', 'workflows', 'Hyein')
filepath = os.path.join(root_dir, "data", "CrossedBarrel.csv")
filedata = pd.read_csv(filepath)
name_X = filedata.columns[:-1].tolist()
name_y = filedata.columns[-1]
df_in = filedata[name_X]
df_out = filedata[[name_y]]
print(f"TARGET: {name_y}")

df_in_final, df_out_final = remove_outliers_iqr(df_in, df_out)

removed_count = len(df_in) - len(df_in_final)
print(f"# of data after removing outliers: {len(df_in_final)} ê°œ ({removed_count} ê°œ ì œê±°ë¨)")

X = df_in_final[name_X].values
y = df_out_final[name_y].values.reshape(-1, 1)

X_temp_denorm, X_test_denorm, y_temp_denorm, y_test_denorm = train_test_split(X, y, test_size=0.2, random_state=42)
X_train_denorm, X_val_denorm, y_train_denorm, y_val_denorm = train_test_split(X_temp_denorm, y_temp_denorm, test_size=0.2,
                                                  random_state=42)
print(f"Train set: {len(X_train_denorm)} ({len(X_train_denorm) / len(X) * 100:.1f}%)")
print(f"Validation set: {len(X_val_denorm)} ({len(X_val_denorm) / len(X) * 100:.1f}%)")
print(f"Test set: {len(X_test_denorm)} ({len(X_test_denorm) / len(X) * 100:.1f}%)")

scaler_X = MinMaxScaler(feature_range=(0.1, 0.9))
scaler_y = MinMaxScaler(feature_range=(0.1, 0.9))
X_train_norm = scaler_X.fit_transform(X_train_denorm)
y_train_norm = scaler_y.fit_transform(y_train_denorm)
X_val_norm = scaler_X.transform(X_val_denorm)
X_test_norm = scaler_X.transform(X_test_denorm)
y_val_norm = scaler_y.transform(y_val_denorm)
y_test_norm = scaler_y.transform(y_test_denorm)

# 3. ëª¨ë¸ ì„ ì–¸ (ê¸°ë³¸ ì„¤ì •)
# n_estimators: ë‚˜ë¬´ì˜ ê°œìˆ˜ (ë³´í†µ 100~1000)
# learning_rate: í•™ìŠµë¥  (ë³´í†µ 0.01~0.1)
# max_depth: ë‚˜ë¬´ì˜ ê¹Šì´ (ë„ˆë¬´ ê¹Šìœ¼ë©´ ê³¼ì í•©, ë³´í†µ 3~6)
model = XGBRegressor(
    n_estimators=500,
    learning_rate=0.05,
    max_depth=5,
    n_jobs=-1,
    random_state=42,
    early_stopping_rounds=50  # <--- ì´ ì„¤ì •ì´ ìµœì‹  ë²„ì „ì—ì„  ì—¬ê¸°ë¡œ ì™”ìŠµë‹ˆë‹¤
)

# í•™ìŠµ (Fit)
print("í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤...")
model.fit(
    X_train_norm, y_train_norm,
    eval_set=[(X_test_norm, y_test_norm)],
    verbose=False
)

# ê²°ê³¼ í™•ì¸
y_pred_norm = model.predict(X_test_norm)
r2 = r2_score(y_test_norm, y_pred_norm)
mse = mean_squared_error(y_test_norm, y_pred_norm)

print("-" * 30)
print(f"âœ… XGBoost R2 Score: {r2:.4f}")
print(f"ğŸ“‰ MSE (Mean Squared Error): {mse:.4f}")
print("-" * 30)

# 6. (ì¶”ê°€) ì¤‘ìš” ë³€ìˆ˜ í™•ì¸í•˜ê¸°
# ì–´ë–¤ ë³€ìˆ˜ê°€ ì˜ˆì¸¡ì— ê°€ì¥ í° ì˜í–¥ì„ ì¤¬ëŠ”ì§€ ë´…ë‹ˆë‹¤.
# KAN ëª¨ë¸ë§ ì‹œ íŒíŠ¸ê°€ ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
print("Feature Importances:", model.feature_importances_)

#%%
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from SALib.sample import saltelli
from SALib.analyze import sobol

# ==========================================
# 1. SALib ë¬¸ì œ ì •ì˜ (Problem Definition)
# ==========================================
# ëª¨ë¸ì´ í•™ìŠµëœ ì…ë ¥ ë³€ìˆ˜ì˜ ê°œìˆ˜ì™€ ë²”ìœ„ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.
# ì£¼ì˜: ìŠ¤ì¼€ì¼ë§ëœ ë°ì´í„°ë¡œ í•™ìŠµí–ˆë‹¤ë©´, ë²”ìœ„ë„ ê·¸ì— ë§ì¶°ì•¼ í•©ë‹ˆë‹¤.
# ì˜ˆ: MinMaxScaler(-1, 1)ì„ ì¼ë‹¤ë©´ boundsëŠ” [-1, 1] ì´ì–´ì•¼ í•©ë‹ˆë‹¤.

# ë³€ìˆ˜ ê°œìˆ˜ (X_trainì˜ ì»¬ëŸ¼ ìˆ˜)
n_features = X_train_norm.shape[1]

# ë³€ìˆ˜ ì´ë¦„ (ì—†ìœ¼ë©´ ê·¸ëƒ¥ x0, x1... ìœ¼ë¡œ ìƒì„±)
feature_names = [f"Feature {i}" for i in range(n_features)]
# ë§Œì•½ pandas ì»¬ëŸ¼ ì´ë¦„ì´ ìˆë‹¤ë©´: feature_names = list(X.columns)

problem = {
    'num_vars': n_features,
    'names': feature_names,
    'bounds': [[-1, 1]] * n_features  # ëª¨ë“  ë³€ìˆ˜ì˜ ë²”ìœ„ê°€ -1 ~ 1 ì´ë¼ê³  ê°€ì • (ìŠ¤ì¼€ì¼ë§ ë§ì¶¤)
}

# ==========================================
# 2. ìƒ˜í”Œ ë°ì´í„° ìƒì„± (Sample Generation)
# ==========================================
# Nì€ ìƒ˜í”Œë§ ê°œìˆ˜ì…ë‹ˆë‹¤. í´ìˆ˜ë¡ ì •í™•í•˜ì§€ë§Œ ê³„ì‚° ì‹œê°„ì´ ëŠ˜ì–´ë‚©ë‹ˆë‹¤. (ë³´í†µ 1024 ì´ìƒ ê¶Œì¥)
# ì´ ì‹¤í–‰ íšŸìˆ˜ = N * (2 * D + 2)  (DëŠ” ë³€ìˆ˜ ê°œìˆ˜)
N = 1024
X_sobol = saltelli.sample(problem, N, calc_second_order=True)

print(f"ìƒì„±ëœ ìƒ˜í”Œ ê°œìˆ˜: {X_sobol.shape[0]}ê°œ")

# ==========================================
# 3. ëª¨ë¸ ì˜ˆì¸¡ ì‹¤í–‰ (Run Model)
# ==========================================
# ìƒì„±ëœ ìƒ˜í”Œ(X_sobol)ì„ XGBoost ëª¨ë¸ì— ë„£ê³  ì˜ˆì¸¡ê°’(Y)ì„ êµ¬í•©ë‹ˆë‹¤.
# XGBoost predictëŠ” numpy arrayë¥¼ ì˜ ë°›ìœ¼ë¯€ë¡œ ë°”ë¡œ ë„£ìœ¼ë©´ ë©ë‹ˆë‹¤.

Y_sobol = model.predict(X_sobol)

# ==========================================
# 4. Sobol ë¶„ì„ ìˆ˜í–‰ (Analyze)
# ==========================================
# calc_second_order=Trueë©´ ë³€ìˆ˜ ê°„ì˜ ìƒí˜¸ì‘ìš©(Interaction)ê¹Œì§€ ë¶„ì„í•©ë‹ˆë‹¤.
Si = sobol.analyze(problem, Y_sobol, calc_second_order=True)

# ==========================================
# 5. ê²°ê³¼ í™•ì¸ ë° ì‹œê°í™”
# ==========================================

# í…ìŠ¤íŠ¸ë¡œ ì¶œë ¥
print("\n[Sobol Analysis Result]")
total_si = Si['ST'] # Total Effect Index (ì´ ì˜í–¥ë ¥)
first_si = Si['S1'] # First Order Index (ë‹¨ë… ì˜í–¥ë ¥)

results_df = pd.DataFrame({
    'Feature': feature_names,
    'Total_Effect (ST)': total_si,
    'First_Order (S1)': first_si
}).sort_values(by='Total_Effect (ST)', ascending=False)

print(results_df)

# ë§‰ëŒ€ ê·¸ë˜í”„ ê·¸ë¦¬ê¸° (ìƒìœ„ 10ê°œë§Œ)
plt.figure(figsize=(10, 6))
plt.title("Feature Sensitivity (Total Effect Index)")
plt.barh(results_df['Feature'][:10][::-1], results_df['Total_Effect (ST)'][:10][::-1])
plt.xlabel("Total Effect Index (ST)")
plt.show()

#%%
from sklearn.neural_network import MLPRegressor
from sklearn.metrics import r2_score, mean_squared_error

# ==========================================
# 2-Layer NN ëª¨ë¸ ì„¤ì •
# ==========================================
# hidden_layer_sizes=(64, 64): ì²« ë²ˆì§¸ ì€ë‹‰ì¸µ 64ê°œ, ë‘ ë²ˆì§¸ ì€ë‹‰ì¸µ 64ê°œ ë…¸ë“œ
# activation='relu': ê°€ì¥ ë³´í¸ì ì¸ í™œì„±í™” í•¨ìˆ˜
# solver='adam': ê¸°ë³¸ optimizer (ì •ë°€í•œ íŠœë‹ ì‹œ 'lbfgs' ì‚¬ìš© ê°€ëŠ¥)
# alpha=0.0001: L2 ê·œì œ (ê³¼ì í•© ë°©ì§€)
# ==========================================

mlp_model = MLPRegressor(
    hidden_layer_sizes=(64, 64),  # 2ê°œì˜ ì€ë‹‰ì¸µ ì„¤ì •
    activation='relu',
    solver='adam',
    alpha=0.0001,
    batch_size='auto',
    learning_rate_init=0.001,
    max_iter=1000,       # ì¶©ë¶„íˆ í•™ìŠµí•˜ë„ë¡ ë°˜ë³µ íšŸìˆ˜ ì„¤ì •
    early_stopping=True, # ì„±ëŠ¥ í–¥ìƒ ì—†ìœ¼ë©´ ì¡°ê¸° ì¢…ë£Œ
    validation_fraction=0.1,
    random_state=42
)

# í•™ìŠµ (ë°˜ë“œì‹œ ìŠ¤ì¼€ì¼ë§ëœ ë°ì´í„°ë¥¼ ë„£ì–´ì•¼ í•©ë‹ˆë‹¤!)
# ì˜ˆ: X_train_scaled, y_train (yë„ ìŠ¤ì¼€ì¼ë§ ì¶”ì²œ)
print("MLP í•™ìŠµ ì‹œì‘...")
mlp_model.fit(X_train_norm, y_train_norm)

# ì˜ˆì¸¡ ë° í‰ê°€
y_pred_mlp_norm = mlp_model.predict(X_test_norm)

r2_mlp = r2_score(y_test_norm, y_pred_mlp_norm)
mse_mlp = mean_squared_error(y_test_norm, y_pred_mlp_norm)

print("-" * 30)
print(f"ğŸ§  2-Layer NN (MLP) R2 Score: {r2_mlp:.4f}")
print(f"ğŸ“‰ MSE: {mse_mlp:.4f}")
print("-" * 30)

#%%
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from SALib.sample import saltelli
from SALib.analyze import sobol

# ==========================================
# 1. SALib ë¬¸ì œ ì •ì˜ (Problem Definition)
# ==========================================
# ëª¨ë¸ì´ í•™ìŠµëœ ì…ë ¥ ë³€ìˆ˜ì˜ ê°œìˆ˜ì™€ ë²”ìœ„ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.
# ì£¼ì˜: ìŠ¤ì¼€ì¼ë§ëœ ë°ì´í„°ë¡œ í•™ìŠµí–ˆë‹¤ë©´, ë²”ìœ„ë„ ê·¸ì— ë§ì¶°ì•¼ í•©ë‹ˆë‹¤.
# ì˜ˆ: MinMaxScaler(-1, 1)ì„ ì¼ë‹¤ë©´ boundsëŠ” [-1, 1] ì´ì–´ì•¼ í•©ë‹ˆë‹¤.

# ë³€ìˆ˜ ê°œìˆ˜ (X_trainì˜ ì»¬ëŸ¼ ìˆ˜)
n_features = X_train_norm.shape[1]

# ë³€ìˆ˜ ì´ë¦„ (ì—†ìœ¼ë©´ ê·¸ëƒ¥ x0, x1... ìœ¼ë¡œ ìƒì„±)
feature_names = [f"Feature {i}" for i in range(n_features)]
# ë§Œì•½ pandas ì»¬ëŸ¼ ì´ë¦„ì´ ìˆë‹¤ë©´: feature_names = list(X.columns)

problem = {
    'num_vars': n_features,
    'names': feature_names,
    'bounds': [[-1, 1]] * n_features  # ëª¨ë“  ë³€ìˆ˜ì˜ ë²”ìœ„ê°€ -1 ~ 1 ì´ë¼ê³  ê°€ì • (ìŠ¤ì¼€ì¼ë§ ë§ì¶¤)
}

# ==========================================
# 2. ìƒ˜í”Œ ë°ì´í„° ìƒì„± (Sample Generation)
# ==========================================
# Nì€ ìƒ˜í”Œë§ ê°œìˆ˜ì…ë‹ˆë‹¤. í´ìˆ˜ë¡ ì •í™•í•˜ì§€ë§Œ ê³„ì‚° ì‹œê°„ì´ ëŠ˜ì–´ë‚©ë‹ˆë‹¤. (ë³´í†µ 1024 ì´ìƒ ê¶Œì¥)
# ì´ ì‹¤í–‰ íšŸìˆ˜ = N * (2 * D + 2)  (DëŠ” ë³€ìˆ˜ ê°œìˆ˜)
N = 1024
X_sobol = saltelli.sample(problem, N, calc_second_order=True)

print(f"ìƒì„±ëœ ìƒ˜í”Œ ê°œìˆ˜: {X_sobol.shape[0]}ê°œ")

# ==========================================
# 3. ëª¨ë¸ ì˜ˆì¸¡ ì‹¤í–‰ (Run Model)
# ==========================================
# ìƒì„±ëœ ìƒ˜í”Œ(X_sobol)ì„ XGBoost ëª¨ë¸ì— ë„£ê³  ì˜ˆì¸¡ê°’(Y)ì„ êµ¬í•©ë‹ˆë‹¤.
# XGBoost predictëŠ” numpy arrayë¥¼ ì˜ ë°›ìœ¼ë¯€ë¡œ ë°”ë¡œ ë„£ìœ¼ë©´ ë©ë‹ˆë‹¤.

Y_sobol = mlp_model.predict(X_sobol)

# ==========================================
# 4. Sobol ë¶„ì„ ìˆ˜í–‰ (Analyze)
# ==========================================
# calc_second_order=Trueë©´ ë³€ìˆ˜ ê°„ì˜ ìƒí˜¸ì‘ìš©(Interaction)ê¹Œì§€ ë¶„ì„í•©ë‹ˆë‹¤.
Si = sobol.analyze(problem, Y_sobol, calc_second_order=True)

# ==========================================
# 5. ê²°ê³¼ í™•ì¸ ë° ì‹œê°í™”
# ==========================================

# í…ìŠ¤íŠ¸ë¡œ ì¶œë ¥
print("\n[Sobol Analysis Result]")
total_si = Si['ST'] # Total Effect Index (ì´ ì˜í–¥ë ¥)
first_si = Si['S1'] # First Order Index (ë‹¨ë… ì˜í–¥ë ¥)

results_df = pd.DataFrame({
    'Feature': feature_names,
    'Total_Effect (ST)': total_si,
    'First_Order (S1)': first_si
}).sort_values(by='Total_Effect (ST)', ascending=False)

print(results_df)

# ë§‰ëŒ€ ê·¸ë˜í”„ ê·¸ë¦¬ê¸° (ìƒìœ„ 10ê°œë§Œ)
plt.figure(figsize=(10, 6))
plt.title("Feature Sensitivity (Total Effect Index)")
plt.barh(results_df['Feature'][:10][::-1], results_df['Total_Effect (ST)'][:10][::-1])
plt.xlabel("Total Effect Index (ST)")
plt.show()
# %%
# ==========================================
# 6. UMAP Visualization (Compatible with sklearn 1.1.3)
# ==========================================
import umap.umap_ as umap # Explicit import for clarity
import matplotlib.pyplot as plt

# 1. Initialize UMAP
# n_neighbors=30: Good balance for ~1000 data points
# min_dist=0.1: Keeps clusters tight
reducer = umap.UMAP(
    n_neighbors=100,
    min_dist=.9,
    n_components=2,
    metric='chebyshev',
    random_state=42
)

# 2. Fit and Transform
# We use the normalized training data you prepared earlier
embedding = reducer.fit_transform(X_train_norm)

# 3. Visualization
plt.figure(figsize=(10, 8))

# Scatter plot: X axis = UMAP dim 1, Y axis = UMAP dim 2
# Color (c) = Your target variable (y_train_norm)
# This allows you to see if the input features naturally separate the target values.
sc = plt.scatter(
    embedding[:, 0],
    embedding[:, 1],
    c=y_train_norm.ravel(), # Flatten the array for coloring
    cmap='Spectral',
    s=20,
    alpha=0.7
)

plt.colorbar(sc, label=f'Target Value ({name_y})')
plt.title('UMAP Projection colored by Target Value', fontsize=15)
plt.xlabel('UMAP Dimension 1')
plt.ylabel('UMAP Dimension 2')
plt.grid(True, alpha=0.3)
plt.show()
