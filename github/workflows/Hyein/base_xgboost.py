import xgboost as xgb
from xgboost import XGBRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score, mean_squared_error
import pandas as pd
import numpy as np

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from kan.custom_processing import remove_outliers_iqr
import torch
import os

root_dir = os.path.join(os.getcwd(), 'github', 'workflows', 'Hyein')
filepath = os.path.join(root_dir, "data", "CrossedBarrel.csv")
filedata = pd.read_csv(filepath)
name_X = filedata.columns[:-1].tolist()
name_y = filedata.columns[-1]
df_in = filedata[name_X]
df_out = filedata[[name_y]]
print(f"TARGET: {name_y}")

df_in_final, df_out_final = remove_outliers_iqr(df_in, df_out)

removed_count = len(df_in) - len(df_in_final)
print(f"# of data after removing outliers: {len(df_in_final)} ê°œ ({removed_count} ê°œ ì œê±°ë¨)")

X = df_in_final[name_X].values
y = df_out_final[name_y].values.reshape(-1, 1)

X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.2,
                                                  random_state=42)
print(f"Train set: {len(X_train)} ({len(X_train) / len(X) * 100:.1f}%)")
print(f"Validation set: {len(X_val)} ({len(X_val) / len(X) * 100:.1f}%)")
print(f"Test set: {len(X_test)} ({len(X_test) / len(X) * 100:.1f}%)")

# 3. ëª¨ë¸ ì„ ì–¸ (ê¸°ë³¸ ì„¤ì •)
# n_estimators: ë‚˜ë¬´ì˜ ê°œìˆ˜ (ë³´í†µ 100~1000)
# learning_rate: í•™ìŠµë¥  (ë³´í†µ 0.01~0.1)
# max_depth: ë‚˜ë¬´ì˜ ê¹Šì´ (ë„ˆë¬´ ê¹Šìœ¼ë©´ ê³¼ì í•©, ë³´í†µ 3~6)
model = XGBRegressor(
    n_estimators=500,
    learning_rate=0.05,
    max_depth=5,
    n_jobs=-1,
    random_state=42,
    early_stopping_rounds=50  # <--- ì´ ì„¤ì •ì´ ìµœì‹  ë²„ì „ì—ì„  ì—¬ê¸°ë¡œ ì™”ìŠµë‹ˆë‹¤
)

# í•™ìŠµ (Fit)
print("í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤...")
# fit() ì•ˆì—ëŠ” ì´ì œ early_stopping_roundsë¥¼ ì“°ì§€ ì•ŠìŠµë‹ˆë‹¤.
model.fit(
    X_train, y_train,
    eval_set=[(X_test, y_test)], # ê²€ì¦ ë°ì´í„°ëŠ” ì—¬ì „íˆ ì—¬ê¸°ì— í•„ìš”í•©ë‹ˆë‹¤
    verbose=False
)

# ê²°ê³¼ í™•ì¸
y_pred = model.predict(X_test)
r2 = r2_score(y_test, y_pred)
mse = mean_squared_error(y_test, y_pred)

print("-" * 30)
print(f"âœ… XGBoost R2 Score: {r2:.4f}")
print(f"ğŸ“‰ MSE (Mean Squared Error): {mse:.4f}")
print("-" * 30)

# 6. (ì¶”ê°€) ì¤‘ìš” ë³€ìˆ˜ í™•ì¸í•˜ê¸°
# ì–´ë–¤ ë³€ìˆ˜ê°€ ì˜ˆì¸¡ì— ê°€ì¥ í° ì˜í–¥ì„ ì¤¬ëŠ”ì§€ ë´…ë‹ˆë‹¤.
# KAN ëª¨ë¸ë§ ì‹œ íŒíŠ¸ê°€ ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
print("Feature Importances:", model.feature_importances_)