import xgboost as xgb
from xgboost import XGBRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score, mean_squared_error
import pandas as pd
import numpy as np

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from kan.custom_processing import remove_outliers_iqr
import torch
import os

root_dir = os.path.join(os.getcwd(), 'github', 'workflows', 'Hyein')
filepath = os.path.join(root_dir, "data", "CrossedBarrel.csv")
filedata = pd.read_csv(filepath)
name_X = filedata.columns[:-1].tolist()
name_y = filedata.columns[-1]
df_in = filedata[name_X]
df_out = filedata[[name_y]]
print(f"TARGET: {name_y}")

df_in_final, df_out_final = remove_outliers_iqr(df_in, df_out)

removed_count = len(df_in) - len(df_in_final)
print(f"# of data after removing outliers: {len(df_in_final)} ê°œ ({removed_count} ê°œ ì œê±°ë¨)")

X = df_in_final[name_X].values
y = df_out_final[name_y].values.reshape(-1, 1)

X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.2,
                                                  random_state=42)
print(f"Train set: {len(X_train)} ({len(X_train) / len(X) * 100:.1f}%)")
print(f"Validation set: {len(X_val)} ({len(X_val) / len(X) * 100:.1f}%)")
print(f"Test set: {len(X_test)} ({len(X_test) / len(X) * 100:.1f}%)")

scaler_X = MinMaxScaler(feature_range=(0.1, 0.9))
scaler_y = MinMaxScaler(feature_range=(0.1, 0.9))
X_train_norm = scaler_X.fit_transform(X_train)
y_train_norm = scaler_y.fit_transform(y_train)
X_val_norm = scaler_X.transform(X_val)
X_test_norm = scaler_X.transform(X_test)
y_val_norm = scaler_y.transform(y_val)
y_test_norm = scaler_y.transform(y_test)

# 3. ëª¨ë¸ ì„ ì–¸ (ê¸°ë³¸ ì„¤ì •)
# n_estimators: ë‚˜ë¬´ì˜ ê°œìˆ˜ (ë³´í†µ 100~1000)
# learning_rate: í•™ìŠµë¥  (ë³´í†µ 0.01~0.1)
# max_depth: ë‚˜ë¬´ì˜ ê¹Šì´ (ë„ˆë¬´ ê¹Šìœ¼ë©´ ê³¼ì í•©, ë³´í†µ 3~6)
model = XGBRegressor(
    n_estimators=500,
    learning_rate=0.05,
    max_depth=5,
    n_jobs=-1,
    random_state=42,
    early_stopping_rounds=50  # <--- ì´ ì„¤ì •ì´ ìµœì‹  ë²„ì „ì—ì„  ì—¬ê¸°ë¡œ ì™”ìŠµë‹ˆë‹¤
)

# í•™ìŠµ (Fit)
print("í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤...")
model.fit(
    X_train_norm, y_train_norm,
    eval_set=[(X_test_norm, y_test_norm)],
    verbose=False
)

# ê²°ê³¼ í™•ì¸
y_pred_norm = model.predict(X_test_norm)
r2 = r2_score(y_test_norm, y_pred_norm)
mse = mean_squared_error(y_test_norm, y_pred_norm)

print("-" * 30)
print(f"âœ… XGBoost R2 Score: {r2:.4f}")
print(f"ğŸ“‰ MSE (Mean Squared Error): {mse:.4f}")
print("-" * 30)

# 6. (ì¶”ê°€) ì¤‘ìš” ë³€ìˆ˜ í™•ì¸í•˜ê¸°
# ì–´ë–¤ ë³€ìˆ˜ê°€ ì˜ˆì¸¡ì— ê°€ì¥ í° ì˜í–¥ì„ ì¤¬ëŠ”ì§€ ë´…ë‹ˆë‹¤.
# KAN ëª¨ë¸ë§ ì‹œ íŒíŠ¸ê°€ ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
print("Feature Importances:", model.feature_importances_)

#%%
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from SALib.sample import saltelli
from SALib.analyze import sobol

# ==========================================
# 1. SALib ë¬¸ì œ ì •ì˜ (Problem Definition)
# ==========================================
# ëª¨ë¸ì´ í•™ìŠµëœ ì…ë ¥ ë³€ìˆ˜ì˜ ê°œìˆ˜ì™€ ë²”ìœ„ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.
# ì£¼ì˜: ìŠ¤ì¼€ì¼ë§ëœ ë°ì´í„°ë¡œ í•™ìŠµí–ˆë‹¤ë©´, ë²”ìœ„ë„ ê·¸ì— ë§ì¶°ì•¼ í•©ë‹ˆë‹¤.
# ì˜ˆ: MinMaxScaler(-1, 1)ì„ ì¼ë‹¤ë©´ boundsëŠ” [-1, 1] ì´ì–´ì•¼ í•©ë‹ˆë‹¤.

# ë³€ìˆ˜ ê°œìˆ˜ (X_trainì˜ ì»¬ëŸ¼ ìˆ˜)
n_features = X_train.shape[1]

# ë³€ìˆ˜ ì´ë¦„ (ì—†ìœ¼ë©´ ê·¸ëƒ¥ x0, x1... ìœ¼ë¡œ ìƒì„±)
feature_names = [f"Feature {i}" for i in range(n_features)]
# ë§Œì•½ pandas ì»¬ëŸ¼ ì´ë¦„ì´ ìˆë‹¤ë©´: feature_names = list(X.columns)

problem = {
    'num_vars': n_features,
    'names': feature_names,
    'bounds': [[-1, 1]] * n_features  # ëª¨ë“  ë³€ìˆ˜ì˜ ë²”ìœ„ê°€ -1 ~ 1 ì´ë¼ê³  ê°€ì • (ìŠ¤ì¼€ì¼ë§ ë§ì¶¤)
}

# ==========================================
# 2. ìƒ˜í”Œ ë°ì´í„° ìƒì„± (Sample Generation)
# ==========================================
# Nì€ ìƒ˜í”Œë§ ê°œìˆ˜ì…ë‹ˆë‹¤. í´ìˆ˜ë¡ ì •í™•í•˜ì§€ë§Œ ê³„ì‚° ì‹œê°„ì´ ëŠ˜ì–´ë‚©ë‹ˆë‹¤. (ë³´í†µ 1024 ì´ìƒ ê¶Œì¥)
# ì´ ì‹¤í–‰ íšŸìˆ˜ = N * (2 * D + 2)  (DëŠ” ë³€ìˆ˜ ê°œìˆ˜)
N = 1024
X_sobol = saltelli.sample(problem, N, calc_second_order=True)

print(f"ìƒì„±ëœ ìƒ˜í”Œ ê°œìˆ˜: {X_sobol.shape[0]}ê°œ")

# ==========================================
# 3. ëª¨ë¸ ì˜ˆì¸¡ ì‹¤í–‰ (Run Model)
# ==========================================
# ìƒì„±ëœ ìƒ˜í”Œ(X_sobol)ì„ XGBoost ëª¨ë¸ì— ë„£ê³  ì˜ˆì¸¡ê°’(Y)ì„ êµ¬í•©ë‹ˆë‹¤.
# XGBoost predictëŠ” numpy arrayë¥¼ ì˜ ë°›ìœ¼ë¯€ë¡œ ë°”ë¡œ ë„£ìœ¼ë©´ ë©ë‹ˆë‹¤.

Y_sobol = model.predict(X_sobol)

# ==========================================
# 4. Sobol ë¶„ì„ ìˆ˜í–‰ (Analyze)
# ==========================================
# calc_second_order=Trueë©´ ë³€ìˆ˜ ê°„ì˜ ìƒí˜¸ì‘ìš©(Interaction)ê¹Œì§€ ë¶„ì„í•©ë‹ˆë‹¤.
Si = sobol.analyze(problem, Y_sobol, calc_second_order=True)

# ==========================================
# 5. ê²°ê³¼ í™•ì¸ ë° ì‹œê°í™”
# ==========================================

# í…ìŠ¤íŠ¸ë¡œ ì¶œë ¥
print("\n[Sobol Analysis Result]")
total_si = Si['ST'] # Total Effect Index (ì´ ì˜í–¥ë ¥)
first_si = Si['S1'] # First Order Index (ë‹¨ë… ì˜í–¥ë ¥)

results_df = pd.DataFrame({
    'Feature': feature_names,
    'Total_Effect (ST)': total_si,
    'First_Order (S1)': first_si
}).sort_values(by='Total_Effect (ST)', ascending=False)

print(results_df)

# ë§‰ëŒ€ ê·¸ë˜í”„ ê·¸ë¦¬ê¸° (ìƒìœ„ 10ê°œë§Œ)
plt.figure(figsize=(10, 6))
plt.title("Feature Sensitivity (Total Effect Index)")
plt.barh(results_df['Feature'][:10][::-1], results_df['Total_Effect (ST)'][:10][::-1])
plt.xlabel("Total Effect Index (ST)")
plt.show()