#%%
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from kan.custom_processing import remove_outliers_iqr
import torch
import os
import pandas as pd

data_name = "CrossedBarrel"

root_dir = os.path.join(os.getcwd(), 'github', 'workflows', 'Hyein')
filepath = os.path.join(root_dir, "data", f"{data_name}.csv")
savepath = os.path.join(root_dir, "nn_models")

filedata = pd.read_csv(filepath)
name_X = filedata.columns[:-1].tolist()
name_y = filedata.columns[-1]
df_in = filedata[name_X]
df_out = filedata[[name_y]]
print(f"TARGET: {name_y}")

df_in_final, df_out_final = remove_outliers_iqr(df_in, df_out)

removed_count = len(df_in) - len(df_in_final)
print(f"# of data after removing outliers: {len(df_in_final)} ê°œ ({removed_count} ê°œ ì œê±°ë¨)")

X = df_in_final[name_X].values
y = df_out_final[name_y].values.reshape(-1, 1)

X_temp_denorm, X_test_denorm, y_temp_denorm, y_test_denorm = train_test_split(X, y, test_size=0.2, random_state=42)
X_train_denorm, X_val_denorm, y_train_denorm, y_val_denorm = train_test_split(X_temp_denorm, y_temp_denorm, test_size=0.2,
                                                  random_state=42)
print(f"Train set: {len(X_train_denorm)} ({len(X_train_denorm) / len(X) * 100:.1f}%)")
print(f"Validation set: {len(X_val_denorm)} ({len(X_val_denorm) / len(X) * 100:.1f}%)")
print(f"Test set: {len(X_test_denorm)} ({len(X_test_denorm) / len(X) * 100:.1f}%)")

scaler_X = MinMaxScaler(feature_range=(0.1, 0.9))
scaler_y = MinMaxScaler(feature_range=(0.1, 0.9))
X_train_norm = scaler_X.fit_transform(X_train_denorm)
y_train_norm = scaler_y.fit_transform(y_train_denorm)
X_val_norm = scaler_X.transform(X_val_denorm)
X_test_norm = scaler_X.transform(X_test_denorm)
y_val_norm = scaler_y.transform(y_val_denorm)
y_test_norm = scaler_y.transform(y_test_denorm)

#%
from sklearn.neural_network import MLPRegressor
from sklearn.model_selection import RandomizedSearchCV
from sklearn.metrics import r2_score
import numpy as np

# ==========================================
# 1. íƒìƒ‰í•  íŒŒë¼ë¯¸í„° ë²”ìœ„ ì„¤ì • (Dictionary)
# ==========================================
param_distributions = {
    # ì€ë‹‰ì¸µ êµ¬ì¡°: (ë…¸ë“œìˆ˜, ë…¸ë“œìˆ˜) -> 2ì¸µ êµ¬ì¡° ìœ„ì£¼ë¡œ í…ŒìŠ¤íŠ¸
    'hidden_layer_sizes': [
        (64, 64),          # ê¸°ë³¸ 2ì¸µ
        (128, 64),         # ì•ì´ ë” ë„“ì€ 2ì¸µ
        (100, 100),        # ë„“ì€ 2ì¸µ
        (64, 32, 16),      # (ì°¸ê³ ìš©) 3ì¸µ êµ¬ì¡°ë„ ìŠ¬ì© ë„£ì–´ë´„
        (128,)             # (ì°¸ê³ ìš©) ì•„ì£¼ ë„“ì€ 1ì¸µ
    ],
    # í™œì„±í™” í•¨ìˆ˜: tanhëŠ” ë¶€ë“œëŸ¬ìš´ ê³¡ì„ (ê³µí•™ ë°ì´í„°)ì— ìœ ë¦¬í•  ìˆ˜ ìˆìŒ
    'activation': ['relu', 'tanh'],
    # Optimizer: lbfgsëŠ” ë°ì´í„°ê°€ ìˆ˜ì²œ ê°œ ì´ë‚´ì¼ ë•Œ ìˆ˜ë ´ ì†ë„ì™€ ì •í™•ë„ê°€ ë§¤ìš° ì¢‹ìŒ
    'solver': ['adam', 'lbfgs'],
    # ê·œì œ(L2) ê°•ë„: ë†’ì„ìˆ˜ë¡ ì‹ì„ ë‹¨ìˆœí•˜ê²Œ ë§Œë“¦
    'alpha': [0.0001, 0.001, 0.01, 0.1],
    # í•™ìŠµë¥  (Adamì¼ ë•Œë§Œ ì ìš©ë¨)
    'learning_rate_init': [0.001, 0.01, 0.0005]
}

# ==========================================
# 2. ê¸°ë³¸ ëª¨ë¸ ë° íŠœë‹ ê°ì²´ ì„¤ì •
# ==========================================
# max_iterë¥¼ ë„‰ë„‰í•˜ê²Œ ì£¼ì–´ ìˆ˜ë ´ ê²½ê³ (ConvergenceWarning) ë°©ì§€
mlp = MLPRegressor(max_iter=100000, random_state=42)

# RandomizedSearchCV ì„¤ì •
# n_iter=20: ì´ 20ë²ˆì˜ ì¡°í•©ì„ ëœë¤ìœ¼ë¡œ ë½‘ì•„ì„œ í…ŒìŠ¤íŠ¸ (ì‹œê°„ ì¡°ì ˆ ê°€ëŠ¥)
# cv=3: 3-Fold êµì°¨ ê²€ì¦ (ë°ì´í„°ë¥¼ 3ê°œë¡œ ìª¼ê°œì„œ ê²€ì¦)
search = RandomizedSearchCV(
    estimator=mlp,
    param_distributions=param_distributions,
    n_iter=200,     # ì‹œë„í•  ì¡°í•©ì˜ ê°œìˆ˜ (ë§ì„ìˆ˜ë¡ ì¢‹ì§€ë§Œ ëŠë ¤ì§)
    cv=3,          # êµì°¨ ê²€ì¦ íšŸìˆ˜
    scoring='r2',  # í‰ê°€ì§€í‘œ: R2 Score
    n_jobs=-1,     # CPU ë³‘ë ¬ ì²˜ë¦¬ (ì†ë„ í–¥ìƒ)
    verbose=1,     # ì§„í–‰ ìƒí™© ì¶œë ¥
    random_state=42
)

# ==========================================
# 3. íŠœë‹ ì‹¤í–‰ (Fitting)
# ==========================================
print("ğŸš€ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ì‹œì‘... (ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”)")
search.fit(X_train_norm, y_train_norm.ravel())

# ==========================================
# 4. ê²°ê³¼ í™•ì¸
# ==========================================
print("\n" + "="*40)
print(f"ğŸ† ìµœì  íŒŒë¼ë¯¸í„°: {search.best_params_}")
print(f"â­ï¸ ìµœê³  êµì°¨ê²€ì¦ ì ìˆ˜ (R2): {search.best_score_:.4f}")
print("="*40)

# ìµœì  ëª¨ë¸ë¡œ í…ŒìŠ¤íŠ¸ ë°ì´í„° ìµœì¢… í‰ê°€
best_model = search.best_estimator_
y_pred = best_model

#%
import joblib
from sklearn.neural_network import MLPRegressor
from sklearn.model_selection import RandomizedSearchCV

# ... (ì•ë¶€ë¶„ì˜ ë°ì´í„° ë¡œë“œ, ìŠ¤ì¼€ì¼ë§, íŠœë‹ ì½”ë“œëŠ” ë™ì¼) ...

# 1. RandomizedSearchCV ì‹¤í–‰ (ê°€ì •)
# search.fit(X_train_scaled, y_train)

# 2. ìµœì  ëª¨ë¸ ì¶”ì¶œ
best_model = search.best_estimator_

# 3. ëª¨ë¸ ë° ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥
# ëª¨ë¸ ì €ì¥ (.pkl íŒŒì¼)
joblib.dump(best_model, os.path.join(savepath, f'{data_name}_best_mlp_model.pkl'))

# ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥ (ë§¤ìš° ì¤‘ìš”! ë‚˜ì¤‘ì— ìƒˆ ë°ì´í„°ë„ ì´ê±¸ë¡œ ë³€í™˜í•´ì•¼ í•¨)
# (ì½”ë“œ ì•ë¶€ë¶„ì—ì„œ ì •ì˜í•œ scaler ë³€ìˆ˜ë¥¼ ì €ì¥í•©ë‹ˆë‹¤)
joblib.dump(scaler_y, os.path.join(savepath, f'{data_name}_mlp_scaler_y.pkl'))
joblib.dump(scaler_X, os.path.join(savepath, f'{data_name}_mlp_scaler_X.pkl'))

print("ğŸ’¾ ëª¨ë¸ê³¼ ìŠ¤ì¼€ì¼ëŸ¬ê°€ ì„±ê³µì ìœ¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!")
print(" - ëª¨ë¸ íŒŒì¼: best_mlp_model.pkl")
print(" - ìŠ¤ì¼€ì¼ëŸ¬ íŒŒì¼: mlp_scaler.pkl")

#%%
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from SALib.sample import saltelli
from SALib.analyze import sobol

# ==========================================
# 1. SALib ë¬¸ì œ ì •ì˜ (Problem Definition)
# ==========================================
# ëª¨ë¸ì´ í•™ìŠµëœ ì…ë ¥ ë³€ìˆ˜ì˜ ê°œìˆ˜ì™€ ë²”ìœ„ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.
# ì£¼ì˜: ìŠ¤ì¼€ì¼ë§ëœ ë°ì´í„°ë¡œ í•™ìŠµí–ˆë‹¤ë©´, ë²”ìœ„ë„ ê·¸ì— ë§ì¶°ì•¼ í•©ë‹ˆë‹¤.
# ì˜ˆ: MinMaxScaler(-1, 1)ì„ ì¼ë‹¤ë©´ boundsëŠ” [-1, 1] ì´ì–´ì•¼ í•©ë‹ˆë‹¤.

# ë³€ìˆ˜ ê°œìˆ˜ (X_trainì˜ ì»¬ëŸ¼ ìˆ˜)
n_features = X_train_norm.shape[1]

# ë³€ìˆ˜ ì´ë¦„ (ì—†ìœ¼ë©´ ê·¸ëƒ¥ x0, x1... ìœ¼ë¡œ ìƒì„±)
feature_names = [f"Feature {i}" for i in range(n_features)]
# ë§Œì•½ pandas ì»¬ëŸ¼ ì´ë¦„ì´ ìˆë‹¤ë©´: feature_names = list(X.columns)

problem = {
    'num_vars': n_features,
    'names': feature_names,
    'bounds': [[-1, 1]] * n_features  # ëª¨ë“  ë³€ìˆ˜ì˜ ë²”ìœ„ê°€ -1 ~ 1 ì´ë¼ê³  ê°€ì • (ìŠ¤ì¼€ì¼ë§ ë§ì¶¤)
}

# ==========================================
# 2. ìƒ˜í”Œ ë°ì´í„° ìƒì„± (Sample Generation)
# ==========================================
# Nì€ ìƒ˜í”Œë§ ê°œìˆ˜ì…ë‹ˆë‹¤. í´ìˆ˜ë¡ ì •í™•í•˜ì§€ë§Œ ê³„ì‚° ì‹œê°„ì´ ëŠ˜ì–´ë‚©ë‹ˆë‹¤. (ë³´í†µ 1024 ì´ìƒ ê¶Œì¥)
# ì´ ì‹¤í–‰ íšŸìˆ˜ = N * (2 * D + 2)  (DëŠ” ë³€ìˆ˜ ê°œìˆ˜)
N = 1024
X_sobol = saltelli.sample(problem, N, calc_second_order=True)

print(f"ìƒì„±ëœ ìƒ˜í”Œ ê°œìˆ˜: {X_sobol.shape[0]}ê°œ")

# ==========================================
# 3. ëª¨ë¸ ì˜ˆì¸¡ ì‹¤í–‰ (Run Model)
# ==========================================
# ìƒì„±ëœ ìƒ˜í”Œ(X_sobol)ì„ XGBoost ëª¨ë¸ì— ë„£ê³  ì˜ˆì¸¡ê°’(Y)ì„ êµ¬í•©ë‹ˆë‹¤.
# XGBoost predictëŠ” numpy arrayë¥¼ ì˜ ë°›ìœ¼ë¯€ë¡œ ë°”ë¡œ ë„£ìœ¼ë©´ ë©ë‹ˆë‹¤.

Y_sobol = best_model.predict(X_sobol)

# ==========================================
# 4. Sobol ë¶„ì„ ìˆ˜í–‰ (Analyze)
# ==========================================
# calc_second_order=Trueë©´ ë³€ìˆ˜ ê°„ì˜ ìƒí˜¸ì‘ìš©(Interaction)ê¹Œì§€ ë¶„ì„í•©ë‹ˆë‹¤.
Si = sobol.analyze(problem, Y_sobol, calc_second_order=True)

# ==========================================
# 5. ê²°ê³¼ í™•ì¸ ë° ì‹œê°í™”
# ==========================================

# í…ìŠ¤íŠ¸ë¡œ ì¶œë ¥
print("\n[Sobol Analysis Result]")
total_si = Si['ST'] # Total Effect Index (ì´ ì˜í–¥ë ¥)
first_si = Si['S1'] # First Order Index (ë‹¨ë… ì˜í–¥ë ¥)

results_df = pd.DataFrame({
    'Feature': feature_names,
    'Total_Effect (ST)': total_si,
    'First_Order (S1)': first_si
}).sort_values(by='Total_Effect (ST)', ascending=False)

print(results_df)

# ë§‰ëŒ€ ê·¸ë˜í”„ ê·¸ë¦¬ê¸° (ìƒìœ„ 10ê°œë§Œ)
plt.figure(figsize=(10, 6))
plt.title("Feature Sensitivity (Total Effect Index)")
plt.barh(results_df['Feature'][:10][::-1], results_df['Total_Effect (ST)'][:10][::-1])
plt.xlabel("Total Effect Index (ST)")
plt.savefig(os.path.join(savepath, f"{data_name}_sobol_analysis.png"))
plt.show()
